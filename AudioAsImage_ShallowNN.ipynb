{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AudioAsImage_ShallowNN.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MoltenMuffins/IndoorAudioClassifier/blob/master/AudioAsImage_ShallowNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "gHAnY5wTTHnH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "VGG model trained on self created database of sounds from freesound\n",
        "\n",
        "Classes: Speech, Music, Water, Door Sounds, Car horn, Glass Breaking"
      ]
    },
    {
      "metadata": {
        "id": "al4pm2TdALkL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 0. Boilerplate Code"
      ]
    },
    {
      "metadata": {
        "id": "MedW0Pm5aLQu",
        "colab_type": "code",
        "outputId": "738e84c0-61df-4af1-e122-a90c09831465",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "#test for gpu\n",
        "import tensorflow as tf\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "X5HOmbBTALEt",
        "colab_type": "code",
        "outputId": "c04aab04-f3df-477c-ccea-94571770c452",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "#imports here\n",
        "import os\n",
        "import librosa\n",
        "from tensorflow.keras.models import Model,load_model,Sequential\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras import backend as K\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "tf.VERSION"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1.12.0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "metadata": {
        "id": "6DOkTa9JAQLE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 1. Download Dataset\n",
        "\n",
        "We use a small labeled dataset generated via the freesound api and passed through an audio tokeniser to shorten exceedingly long sound files.\n",
        "\n",
        "It has the following file structure: `Dataset/Train/{CLASS_LABEL}/{FILENAME}.jpg`"
      ]
    },
    {
      "metadata": {
        "id": "dHuoM7-QSoGQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Download Dataset from dropbox\n",
        "!wget -qq https://www.dropbox.com/s/rv7xzjyvae0nabt/Data.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hNKXSSraMH4z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ef25f793-0331-477d-a6c6-c3bc9b83dfe6"
      },
      "cell_type": "code",
      "source": [
        "#Unzip and delete zip file\n",
        "!unzip -qq Data.zip\n",
        "!rm Data.zip\n",
        "!ls"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Data  __MACOSX\tsample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Cm5uJci_Ard_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 2. Prepare Dataset for Model\n",
        "\n",
        "We use glob to get lists of the files in the directories and then convert them into dataframes and add in class numbers.\n",
        "\n",
        "We also then split them up so we have 10% for a testing set and the rest for training.\n",
        "\n",
        "Finally we randomly shuffle them up"
      ]
    },
    {
      "metadata": {
        "id": "z3FiJfO5OrQa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "path = \"./Data/\"\n",
        "train_data_dir = \"./Data/Train/\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ezkNj9j4Pye_",
        "colab_type": "code",
        "outputId": "7476df7e-1ebd-4f5b-c4e3-62763106173f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import glob\n",
        "\n",
        "filenames_n0 = glob.glob('./Data/Train/car horn/*.png')\n",
        "filenames_n1 = glob.glob('./Data/Train/door/*.png')\n",
        "filenames_n2 = glob.glob('./Data/Train/glass break/*.png')\n",
        "filenames_n3 = glob.glob('./Data/Train/music/*.png')\n",
        "filenames_n4 = glob.glob('./Data/Train/speech/*.png')\n",
        "filenames_n5 = glob.glob('./Data/Train/water/*.png')\n",
        "\n",
        "\n",
        "names = ['car horn', 'door', 'glass break', 'music', 'speech', 'water']\n",
        "\n",
        "len(filenames_n0)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "52"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "metadata": {
        "id": "BFKrmMDxQMlo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Make a dataframe based on the filenames\n",
        "df = pd.DataFrame(filenames_n0, columns = [\"filename\"])\n",
        "df2 = pd.DataFrame(filenames_n1, columns = [\"filename\"])\n",
        "df3 = pd.DataFrame(filenames_n2, columns = [\"filename\"])\n",
        "df4 = pd.DataFrame(filenames_n3, columns = [\"filename\"])\n",
        "df5 = pd.DataFrame(filenames_n4, columns = [\"filename\"])\n",
        "df6 = pd.DataFrame(filenames_n5, columns = [\"filename\"])\n",
        "\n",
        "\n",
        "# Add Class columns \n",
        "df['class'] = pd.Series([0 for x in range(len(df.index))], index=df.index)\n",
        "df2['class'] = pd.Series([1 for x in range(len(df2.index))], index=df2.index)\n",
        "df3['class'] = pd.Series([2 for x in range(len(df3.index))], index=df3.index)\n",
        "df4['class'] = pd.Series([3 for x in range(len(df4.index))], index=df4.index)\n",
        "df5['class'] = pd.Series([4 for x in range(len(df5.index))], index=df5.index)\n",
        "df6['class'] = pd.Series([5 for x in range(len(df6.index))], index=df6.index)\n",
        "\n",
        "\n",
        "# Split into train and validation sets\n",
        "train_set_percentage = .9\n",
        "\n",
        "#1\n",
        "train_df = df[:int(len(df)*train_set_percentage)]\n",
        "val_df = df[int(len(df)*train_set_percentage):]\n",
        "\n",
        "#2\n",
        "train_df2 = df2[:int(len(df2)*train_set_percentage)]\n",
        "val_df2 = df2[int(len(df2)*train_set_percentage):]\n",
        "\n",
        "#3\n",
        "train_df3 = df3[:int(len(df3)*train_set_percentage)]\n",
        "val_df3 = df3[int(len(df3)*train_set_percentage):]\n",
        "\n",
        "#4\n",
        "train_df4 = df4[:int(len(df4)*train_set_percentage)]\n",
        "val_df4 = df4[int(len(df4)*train_set_percentage):]\n",
        "\n",
        "#5\n",
        "train_df5 = df5[:int(len(df5)*train_set_percentage)]\n",
        "val_df5 = df5[int(len(df5)*train_set_percentage):]\n",
        "\n",
        "#6\n",
        "train_df6 = df6[:int(len(df6)*train_set_percentage)]\n",
        "val_df6 = df6[int(len(df6)*train_set_percentage):]\n",
        "\n",
        "df_new_train = pd.concat([train_df, train_df2, train_df3, train_df4, train_df5, train_df6])\n",
        "df_new_val = pd.concat([val_df, val_df2, val_df3, val_df4, val_df5, val_df6])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0Xr6G32PS2R-",
        "colab_type": "code",
        "outputId": "116eb056-a31d-46b0-b1bd-2e9cc56f9684",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 393
        }
      },
      "cell_type": "code",
      "source": [
        "# shuffle dataframes\n",
        "df = df_new_train.sample(frac=1).reset_index(drop=True)\n",
        "df_val = df_new_val.sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "print('number of train files:', len(df))\n",
        "print('number of val files:', len(df_val))\n",
        "df.head(10)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "number of train files: 905\n",
            "number of val files: 104\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>filename</th>\n",
              "      <th>class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>./Data/Train/door/Door Squeak, Normal, A.png</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>./Data/Train/speech/corpspeech_clip2.png</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>./Data/Train/water/water-in-container_clip0.png</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>./Data/Train/speech/tata_stan_brian01_clip5.png</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>./Data/Train/water/water splash 2.png</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>./Data/Train/car horn/car_horn_clip0.png</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>./Data/Train/door/Door Squeak, Normal, C.png</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>./Data/Train/speech/mean_grinch_christmas_01_c...</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>./Data/Train/water/Water Dropping.png</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>./Data/Train/door/door_slams_r_clip0.png</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            filename  class\n",
              "0       ./Data/Train/door/Door Squeak, Normal, A.png      1\n",
              "1           ./Data/Train/speech/corpspeech_clip2.png      4\n",
              "2    ./Data/Train/water/water-in-container_clip0.png      5\n",
              "3    ./Data/Train/speech/tata_stan_brian01_clip5.png      4\n",
              "4              ./Data/Train/water/water splash 2.png      5\n",
              "5           ./Data/Train/car horn/car_horn_clip0.png      0\n",
              "6       ./Data/Train/door/Door Squeak, Normal, C.png      1\n",
              "7  ./Data/Train/speech/mean_grinch_christmas_01_c...      4\n",
              "8              ./Data/Train/water/Water Dropping.png      5\n",
              "9           ./Data/Train/door/door_slams_r_clip0.png      1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "metadata": {
        "id": "mwEGoSWqXb1G",
        "colab_type": "code",
        "outputId": "cafca957-e1fa-420d-d332-728e481639f2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# convert the dataframe into 2 lists to use for filename and labels\n",
        "train_filenames_list = df[\"filename\"].tolist()\n",
        "train_labels_list = df[\"class\"].astype('int32').tolist()\n",
        "\n",
        "# convert the dataframe into 2 lists to use for filename and labels\n",
        "val_filenames_list = df_val[\"filename\"].tolist()\n",
        "val_labels_list = df_val[\"class\"].astype('int32').tolist()\n",
        "\n",
        "#number of classes\n",
        "num_classes = 6\n",
        "\n",
        "df.shape"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(905, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "metadata": {
        "id": "gBzms4rAXskF",
        "colab_type": "code",
        "outputId": "912fa5a6-7cd6-4798-f98a-22e14e38486a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "cell_type": "code",
      "source": [
        "train_filenames_list[:5]"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['./Data/Train/door/Door Squeak, Normal, A.png',\n",
              " './Data/Train/speech/corpspeech_clip2.png',\n",
              " './Data/Train/water/water-in-container_clip0.png',\n",
              " './Data/Train/speech/tata_stan_brian01_clip5.png',\n",
              " './Data/Train/water/water splash 2.png']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "metadata": {
        "id": "S7zf1-KobyL-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 3. Shallow NN"
      ]
    },
    {
      "metadata": {
        "id": "fViP0ZEbAeRT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 3.1 Create Data Pipeline for Our NN"
      ]
    },
    {
      "metadata": {
        "id": "h2X6htO6c7dD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#As our image is greyscale (single channel) we need to do something to it to make it 3 channeled\n",
        "\n",
        "# Reads an image from a file, decodes it into a tensor, and resizes it\n",
        "# to a fixed shape.\n",
        "\n",
        "def _parse_function(filename, label):\n",
        "  image_string = tf.read_file(filename)\n",
        "  image_decoded = tf.image.decode_jpeg(image_string)\n",
        "  label = tf.one_hot(label, num_classes)\n",
        "  return image_decoded, label"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TLtA31kweB1T",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Create vector of filenames from list\n",
        "filenames = tf.constant(train_filenames_list)\n",
        "\n",
        "# Create vector of labels\n",
        "labels = tf.constant(train_labels_list)\n",
        "\n",
        "# Same as above but for validation set\n",
        "val_filenames = tf.constant(val_filenames_list)\n",
        "val_labels = tf.constant(val_labels_list)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ck-ltzcABHlG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 3.2 Asemble Data Pipeline using tf.data for VGG16"
      ]
    },
    {
      "metadata": {
        "id": "bc6Mwp1lFqen",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_dataset = tf.data.Dataset.from_tensor_slices((filenames, labels))\n",
        "train_dataset = train_dataset.map(_parse_function)\n",
        "train_dataset = train_dataset.repeat()\n",
        "train_dataset = train_dataset.batch(32)\n",
        "\n",
        "valid_dataset = tf.data.Dataset.from_tensor_slices((val_filenames, val_labels))\n",
        "valid_dataset = valid_dataset.map(_parse_function)\n",
        "valid_dataset = valid_dataset.repeat()\n",
        "valid_dataset = valid_dataset.batch(32)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SqVOvvvydCaP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Conv2D(16, kernel_size=(5, 5), activation='relu', input_shape=(64, 32, 3)))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Conv2D(16, (5, 5), activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(32, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(num_classes, activation='softmax'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3h5gZCzH_zdy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "opt = tf.train.AdamOptimizer(learning_rate = 0.00001)\n",
        "model.compile(optimizer=opt, loss='categorical_crossentropy',metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ledAEOvsFrJu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 3.3 Run Neural Network"
      ]
    },
    {
      "metadata": {
        "id": "pD7vN-nf_zd0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#60 20 20 rule\n",
        "train_steps = 60\n",
        "val_steps = 20\n",
        "epochs = 30"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dHnxnl2AY6iI",
        "colab_type": "code",
        "outputId": "9dd7938c-1f36-4a13-f326-1aaef785e858",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "#sanity check\n",
        "print(train_dataset)\n",
        "print(valid_dataset)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<BatchDataset shapes: ((?, ?, ?, 3), (?, 6)), types: (tf.uint8, tf.float32)>\n",
            "<BatchDataset shapes: ((?, ?, ?, 3), (?, 6)), types: (tf.uint8, tf.float32)>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "DGGH-czf_zd2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1057
        },
        "outputId": "69c4d43c-e7e2-4dd2-ffc1-5c0ac2cfcf9c"
      },
      "cell_type": "code",
      "source": [
        "# # # Train the model with validation \n",
        "history = model.fit( train_dataset, steps_per_epoch = train_steps,\n",
        "                   epochs = epochs,\n",
        "                   validation_data = valid_dataset,\n",
        "                   validation_steps = val_steps)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "60/60 [==============================] - 4s 71ms/step - loss: 12.8634 - acc: 0.1505 - val_loss: 11.7195 - val_acc: 0.1844\n",
            "Epoch 2/30\n",
            "60/60 [==============================] - 1s 23ms/step - loss: 12.6715 - acc: 0.1573 - val_loss: 11.5299 - val_acc: 0.1625\n",
            "Epoch 3/30\n",
            "60/60 [==============================] - 1s 23ms/step - loss: 12.5516 - acc: 0.1609 - val_loss: 10.8561 - val_acc: 0.1812\n",
            "Epoch 4/30\n",
            "60/60 [==============================] - 1s 23ms/step - loss: 12.0601 - acc: 0.1807 - val_loss: 10.2815 - val_acc: 0.2078\n",
            "Epoch 5/30\n",
            "60/60 [==============================] - 1s 22ms/step - loss: 11.5582 - acc: 0.1943 - val_loss: 9.2368 - val_acc: 0.2703\n",
            "Epoch 6/30\n",
            "60/60 [==============================] - 1s 23ms/step - loss: 11.1319 - acc: 0.2078 - val_loss: 8.8627 - val_acc: 0.2797\n",
            "Epoch 7/30\n",
            "60/60 [==============================] - 1s 22ms/step - loss: 10.7778 - acc: 0.2094 - val_loss: 8.7087 - val_acc: 0.2719\n",
            "Epoch 8/30\n",
            "60/60 [==============================] - 1s 22ms/step - loss: 10.3504 - acc: 0.2042 - val_loss: 8.6010 - val_acc: 0.3078\n",
            "Epoch 9/30\n",
            "60/60 [==============================] - 1s 23ms/step - loss: 9.5892 - acc: 0.2135 - val_loss: 8.0211 - val_acc: 0.2781\n",
            "Epoch 10/30\n",
            "60/60 [==============================] - 1s 22ms/step - loss: 8.0798 - acc: 0.2297 - val_loss: 6.6481 - val_acc: 0.2797\n",
            "Epoch 11/30\n",
            "60/60 [==============================] - 1s 23ms/step - loss: 6.7635 - acc: 0.2411 - val_loss: 4.7327 - val_acc: 0.2875\n",
            "Epoch 12/30\n",
            "60/60 [==============================] - 1s 23ms/step - loss: 5.3839 - acc: 0.2464 - val_loss: 3.6129 - val_acc: 0.2672\n",
            "Epoch 13/30\n",
            "60/60 [==============================] - 1s 23ms/step - loss: 4.4343 - acc: 0.2536 - val_loss: 3.0387 - val_acc: 0.2781\n",
            "Epoch 14/30\n",
            "60/60 [==============================] - 1s 23ms/step - loss: 3.5560 - acc: 0.2604 - val_loss: 2.6916 - val_acc: 0.2656\n",
            "Epoch 15/30\n",
            "60/60 [==============================] - 1s 23ms/step - loss: 2.9975 - acc: 0.2969 - val_loss: 2.5329 - val_acc: 0.2781\n",
            "Epoch 16/30\n",
            "60/60 [==============================] - 1s 23ms/step - loss: 2.5643 - acc: 0.3099 - val_loss: 2.3959 - val_acc: 0.2594\n",
            "Epoch 17/30\n",
            "60/60 [==============================] - 1s 22ms/step - loss: 2.4669 - acc: 0.2880 - val_loss: 2.3003 - val_acc: 0.2484\n",
            "Epoch 18/30\n",
            "60/60 [==============================] - 1s 23ms/step - loss: 2.2506 - acc: 0.3021 - val_loss: 2.2584 - val_acc: 0.2625\n",
            "Epoch 19/30\n",
            "60/60 [==============================] - 1s 23ms/step - loss: 2.1631 - acc: 0.3026 - val_loss: 2.1930 - val_acc: 0.2203\n",
            "Epoch 20/30\n",
            "60/60 [==============================] - 1s 23ms/step - loss: 2.0208 - acc: 0.3000 - val_loss: 2.1431 - val_acc: 0.2203\n",
            "Epoch 21/30\n",
            "60/60 [==============================] - 1s 23ms/step - loss: 2.0086 - acc: 0.2891 - val_loss: 2.1186 - val_acc: 0.2219\n",
            "Epoch 22/30\n",
            "60/60 [==============================] - 1s 23ms/step - loss: 1.9204 - acc: 0.3161 - val_loss: 2.1084 - val_acc: 0.2500\n",
            "Epoch 23/30\n",
            "60/60 [==============================] - 1s 22ms/step - loss: 1.8930 - acc: 0.3130 - val_loss: 2.0858 - val_acc: 0.2578\n",
            "Epoch 24/30\n",
            "60/60 [==============================] - 1s 23ms/step - loss: 1.8554 - acc: 0.3135 - val_loss: 2.0774 - val_acc: 0.2531\n",
            "Epoch 25/30\n",
            "60/60 [==============================] - 1s 23ms/step - loss: 1.8170 - acc: 0.3167 - val_loss: 2.0632 - val_acc: 0.2594\n",
            "Epoch 26/30\n",
            "60/60 [==============================] - 1s 23ms/step - loss: 1.8005 - acc: 0.3146 - val_loss: 2.0288 - val_acc: 0.3094\n",
            "Epoch 27/30\n",
            "60/60 [==============================] - 1s 23ms/step - loss: 1.8005 - acc: 0.3161 - val_loss: 2.0060 - val_acc: 0.2719\n",
            "Epoch 28/30\n",
            "60/60 [==============================] - 1s 23ms/step - loss: 1.8071 - acc: 0.3120 - val_loss: 2.0064 - val_acc: 0.2797\n",
            "Epoch 29/30\n",
            "60/60 [==============================] - 1s 23ms/step - loss: 1.7846 - acc: 0.3120 - val_loss: 1.9790 - val_acc: 0.2750\n",
            "Epoch 30/30\n",
            "60/60 [==============================] - 1s 22ms/step - loss: 1.7460 - acc: 0.3365 - val_loss: 1.9687 - val_acc: 0.2781\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "U0zHompMfviy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        },
        "outputId": "54beb03c-108d-4bda-99c0-c7274d380bd9"
      },
      "cell_type": "code",
      "source": [
        "# # # Let's train it more just to see what happens (will probably overfit given lack of data)\n",
        "epochs = 1000\n",
        "\n",
        "history = model.fit( train_dataset, steps_per_epoch = train_steps,\n",
        "                   epochs = epochs,\n",
        "                   validation_data = valid_dataset,\n",
        "                   validation_steps = val_steps)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "60/60 [==============================] - 1s 23ms/step - loss: 0.6236 - acc: 0.7396 - val_loss: 2.6599 - val_acc: 0.5781\n",
            "Epoch 994/1000\n",
            "60/60 [==============================] - 1s 23ms/step - loss: 0.6159 - acc: 0.7438 - val_loss: 2.6153 - val_acc: 0.5766\n",
            "Epoch 995/1000\n",
            "60/60 [==============================] - 1s 23ms/step - loss: 0.6230 - acc: 0.7375 - val_loss: 2.6351 - val_acc: 0.5797\n",
            "Epoch 996/1000\n",
            "60/60 [==============================] - 1s 23ms/step - loss: 0.6549 - acc: 0.7281 - val_loss: 2.6973 - val_acc: 0.5625\n",
            "Epoch 997/1000\n",
            "60/60 [==============================] - 1s 23ms/step - loss: 0.6366 - acc: 0.7313 - val_loss: 2.7112 - val_acc: 0.5750\n",
            "Epoch 998/1000\n",
            "60/60 [==============================] - 1s 23ms/step - loss: 0.6301 - acc: 0.7385 - val_loss: 2.6790 - val_acc: 0.5797\n",
            "Epoch 999/1000\n",
            "60/60 [==============================] - 1s 23ms/step - loss: 0.6230 - acc: 0.7339 - val_loss: 2.6605 - val_acc: 0.5781\n",
            "Epoch 1000/1000\n",
            "60/60 [==============================] - 1s 23ms/step - loss: 0.6431 - acc: 0.7250 - val_loss: 2.6614 - val_acc: 0.5750\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "HbFMbMV_HsRp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 4. Model Adhoc Test"
      ]
    },
    {
      "metadata": {
        "id": "hbgFaa6HIHdZ",
        "colab_type": "code",
        "outputId": "0cb21611-212d-4dbc-e238-7b65e0a54d37",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81
        }
      },
      "cell_type": "code",
      "source": [
        "image_path = './Data/Test/'\n",
        "\n",
        "from IPython.display import Image\n",
        "\n",
        "#Filenames:'horn.png', 'door.png', 'glass.png', 'music.png', 'speech.png', 'water.png'\n",
        "image_check = 'glass.png'\n",
        "#These files are not present in the train set\n",
        "\n",
        "Image(image_path+image_check)"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAACAAAABACAAAAABe5SpyAAAFO0lEQVR4nCXUzW4dxxEG0K+qq2fu\nHylKsh07drywgGiR7LLJC+b5AgQI4E0QI44lWiYp8t47M91df1nkPMShv11/uVftE69bGWxpHBnJ\nKIA9E5PIqUZxWihzSzfLtHAOR7gncfJD7mgqcpi4cCWaRUBMZSrCREQswUrqAbdW1MPSkQ5oIjIz\nSL5/mU5DCxWVShlgUCRCApEgQF7P9iyk6DdJh/3mFGagDKJxBRHxpS6WHFkdAk9KglFmhA9kRPLT\nUoyap5OU4kYJFBCjcCkApVyP2PtOdabImIFizBXpnBN5AvL95muqwz0NPkCpFqSeUQplQra79bX0\nC8kmZenkVZEOEg72C0CyzuARDdOyRThiWCKSYORBmZDldrlegjQ65QoKBK9JkZTJJTOl7HYnXEkw\nNzlEq2BCsE8+Qmg8Q0qLwTFEl41j8zCLQWHkhGgBiB7Ote91rskU5O5uc0aPCY6eDJGnl8+46CV1\nRV61elDvrAEEkpLkLckBJuIzm54G2CCHTgl4JAA5HfpHxyGq+3QRLhoZXEGsgcUYrJvuPC7NIlw4\no0hlRppyeBBSfv526yyLDzMf2SPJ1KCBzKAE5Db9EiOsVtW09NBwi8EIikxAfqLHX+FqiIQLz15d\n90yIlnw7LU/y8+vz41HVb5imcltlEo/DD7E+dCj5L3+XP2yP9rnbRIwdt8N3Xz19sMOrmv8xN7lI\nlR1/fZWqAbblIPt3xxdGu5TxgjFlu6o8vX11Mw13ogqamdZrEWy+jinX7A9d6PRYKl9stijZyvOE\nbw9P62T8BS9LDJMv7l7mk5LexpWnSq0HYNNXcw/el5v1J344Y58NfWzWrqN/+O8ZvGg8r0ba8Tbl\n3zfnnq3UzhIxEUpdaaKnI46g1ut7eX9XP4VSGWWGqdtc6ISX9ru7172fn+8X+ebV/KXtPa20MgOz\n7P705sexteka3WR9ke1urNP0vK+dvRBC87M99lULOR13ElJXHb2MBHVoMHb1X1M7D+bjNnrTA9vk\nNF3LGC1WJUZrdng3yzz/+Y/fcNvNMppTJ6+mlqbETN7d9ze/flw/nc+6ymne3YyDelOeuzOlXXgi\nTNPN/rfr2lOCVtO1hXXeBopJ8MtHvXt3+pGOud9Clt9PJo4q4jlZgNIj2+5j+dWOl0zIbYDyuBRH\nkhETVyBqexgrPbsk5LZkLdPU0EuNCLCxOZGVg+t+d4ZMxze/1cF1p2BDV1KxpvPr+7/YP9cru7Tp\nPJoq9y2psWMdEYF9fBjxtC4JKXw4PM4FGWPAbB6ZnnFV/4x+DRS+ROJUzoYyHcpcVVAPlcYl2zgy\niOThW47KJ9nNBVEG22LevAc0440B8nUMUmy7rpuHEvfQ4aknOqe+AJDy5uXrS9bq2167EU0UYRZf\nyH05/rhkEb7/9LBsttMxhmp6ws2caL7306MGS+IgEzY6RjCVvtcOTuJ1JK+RlFLVX4pHXrbYMrC5\nRyLHpTR2B0HuxvFLW0Pkxi0t06KE31yvIPaXRJFPN+1CjccUhrboHUaEtc1JyZMQ8o+/Zh1pps11\nDXPvqQG/lUZJmSnv+6WVstTARmxxYdcBHsqW/x9mm1PbZgivIemZ4iBIhxFTEuTtaV0/7A/DpBgj\nvAKZ7B47RCBJ9jzRm9yO8DW3V8+7YqZ20rVrGAeRbBVRjThs9PARTIv62T2IDACYhcu+yH4utRSv\nt+7CmVSkSAEIUq+28bL5uJJtiaWbD82Ag4OQkLKRpSKDPNm7RkZykCdiRBKJT+W77akfCrGtC0lz\nOMMlAYCI/gdapUtH8+nGfwAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "metadata": {
        "id": "52mLQgEKITV8",
        "colab_type": "code",
        "outputId": "9304162b-1140-4228-d581-f505a99ea9ae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "from keras.preprocessing import image\n",
        "from keras.applications.imagenet_utils import preprocess_input\n",
        "import numpy as np\n",
        "\n",
        "img_path = os.path.join(image_path, image_check)\n",
        "print(img_path)\n",
        "img = image.load_img(img_path, target_size=(64,32))\n",
        "x = image.img_to_array(img)\n",
        "x = np.expand_dims(x, axis=0)\n",
        "x = preprocess_input(x)\n",
        "print('Input image shape:', x.shape)"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "./Data/Test/glass.png\n",
            "Input image shape: (1, 64, 32, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "weRy0VLMITbZ",
        "colab_type": "code",
        "outputId": "85543f92-de3b-4770-bacc-617f7a350e3f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "preds = model.predict(x)\n",
        "print(preds)"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.0000000e+00 0.0000000e+00 0.0000000e+00 2.8362808e-19 1.0000000e+00\n",
            "  0.0000000e+00]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "XR9F16FHTeQV",
        "colab_type": "code",
        "outputId": "8773f7c4-86f1-4a8b-de12-94b0589bbf0c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "cell_type": "code",
      "source": [
        "import operator\n",
        "def decode(preds):\n",
        "  names = ['car horn', 'door', 'glass break', 'music', 'speech', 'water']\n",
        "  probs = preds[0].tolist()\n",
        "  dictionary = dict(zip(names, probs))  \n",
        "  return sorted(dictionary.items(), key=operator.itemgetter(1), reverse=True)\n",
        "  \n",
        "decode(preds)"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('speech', 1.0),\n",
              " ('music', 2.836280828291863e-19),\n",
              " ('car horn', 0.0),\n",
              " ('door', 0.0),\n",
              " ('glass break', 0.0),\n",
              " ('water', 0.0)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    }
  ]
}